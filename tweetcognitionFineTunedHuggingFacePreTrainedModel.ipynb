{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050a16a2-0353-48aa-b91a-0a96d333ff50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pywidgets (/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install evaluate --quiet\n",
    "!pip install openpyxl  --quiet\n",
    "!pip install torch  --quiet\n",
    "!pip install -U scikit-learn scipy matplotlib --quiet\n",
    "!pip install --upgrade accelerate  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71782988-136c-4fc8-bd74-aa410fc838f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback, set_seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "import datasets\n",
    "import math\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58eae7c6-b01c-4739-af43-6d22aa28e0c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enthusiasm is rare, Endurance is rare.</td>\n",
       "      <td>GRADUATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>That amazing moment!</td>\n",
       "      <td>GRADUATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Work hard. Stay humble.;.;.;#graduating #gradu...</td>\n",
       "      <td>GRADUATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nomi_9867 @BVBoni17 Education is key üñçÔ∏è</td>\n",
       "      <td>GRADUATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big journey begins with small steps.;.;.;#gra...</td>\n",
       "      <td>GRADUATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>Was just told that my dear friend, Dr. Ramin O...</td>\n",
       "      <td>DEATH_OF_A_LOVED_ONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>@sy_fyn_ity I'm so sorry, how awful. My dad di...</td>\n",
       "      <td>DEATH_OF_A_LOVED_ONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>my other grandpa just died by a heart attack</td>\n",
       "      <td>DEATH_OF_A_LOVED_ONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>My grandma died last night. I knew it was comi...</td>\n",
       "      <td>DEATH_OF_A_LOVED_ONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>I'm so upset because finally everything was go...</td>\n",
       "      <td>DEATH_OF_A_LOVED_ONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>876 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text                 label\n",
       "0               Enthusiasm is rare, Endurance is rare.            GRADUATION\n",
       "1                                 That amazing moment!            GRADUATION\n",
       "2    Work hard. Stay humble.;.;.;#graduating #gradu...            GRADUATION\n",
       "3             @nomi_9867 @BVBoni17 Education is key üñçÔ∏è            GRADUATION\n",
       "4     Big journey begins with small steps.;.;.;#gra...            GRADUATION\n",
       "..                                                 ...                   ...\n",
       "871  Was just told that my dear friend, Dr. Ramin O...  DEATH_OF_A_LOVED_ONE\n",
       "872  @sy_fyn_ity I'm so sorry, how awful. My dad di...  DEATH_OF_A_LOVED_ONE\n",
       "873      my other grandpa just died by a heart attack   DEATH_OF_A_LOVED_ONE\n",
       "874  My grandma died last night. I knew it was comi...  DEATH_OF_A_LOVED_ONE\n",
       "875  I'm so upset because finally everything was go...  DEATH_OF_A_LOVED_ONE\n",
       "\n",
       "[876 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CLEANUP\n",
    "\n",
    "\n",
    "# Load labeled dataset of tweets related to specific life events\n",
    "df = pd.read_excel('dataset/LabeledTweets.xlsx', names=['text', 'label'])\n",
    "print(df.isna().sum())  # print the number of NaN values in each column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9660a83b-7efb-4cfd-83b6-da557ce3bbec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>It's time to share my story and raise awarenes...</td>\n",
       "      <td>ADDICTION_RECOVERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>To get to marry my best friend is a dream tha...</td>\n",
       "      <td>WEDDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>It's not always possible to offer promotions ...</td>\n",
       "      <td>WORK_PROMOTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>RT @HarkiratKukreja: We are so proud of our d...</td>\n",
       "      <td>GRADUATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Congratulations Brystal!!! #classof2023 https:...</td>\n",
       "      <td>GRADUATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Lost my job due to company downsizing. It's a ...</td>\n",
       "      <td>FIRED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>With mortarboard on my head and a diploma in m...</td>\n",
       "      <td>GRADUATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Congratulations to the graduating Class of 20...</td>\n",
       "      <td>GRADUATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Resignation letter sent! I'm officially quitti...</td>\n",
       "      <td>QUIT_JOB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Leaving footprints around the world: I'm off t...</td>\n",
       "      <td>MAJOR_TRAVEL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>876 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text               label\n",
       "781  It's time to share my story and raise awarenes...  ADDICTION_RECOVERY\n",
       "252   To get to marry my best friend is a dream tha...             WEDDING\n",
       "178   It's not always possible to offer promotions ...      WORK_PROMOTION\n",
       "69    RT @HarkiratKukreja: We are so proud of our d...          GRADUATION\n",
       "52   Congratulations Brystal!!! #classof2023 https:...          GRADUATION\n",
       "..                                                 ...                 ...\n",
       "339  Lost my job due to company downsizing. It's a ...               FIRED\n",
       "653  With mortarboard on my head and a diploma in m...          GRADUATION\n",
       "21    Congratulations to the graduating Class of 20...          GRADUATION\n",
       "692  Resignation letter sent! I'm officially quitti...            QUIT_JOB\n",
       "598  Leaving footprints around the world: I'm off t...        MAJOR_TRAVEL\n",
       "\n",
       "[876 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e8829e-d901-48b8-bf1d-19979b6dbc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-5\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "MODEL = 'cardiffnlp/twitter-roberta-base-sep2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11fac743-811d-4121-bd9f-eed93cbfed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set transformers seed\n",
    "seed = 223\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83d780af-4b9d-4395-978b-98c3e7228b46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25553/3361373016.py\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Transform the string labels into integer representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_dict = {}\n",
    "\n",
    "train_df = df.head(math.trunc(len(df)*0.8))\n",
    "test_df = df.tail(math.trunc(len(df)*0.2))\n",
    "val_df = test_df.tail(BATCH_SIZE*2)\n",
    "\n",
    "# Convert the subset DataFrame to a dictionary\n",
    "train_dict = train_df.to_dict(orient='list')\n",
    "test_dict = test_df.to_dict(orient='list')\n",
    "val_dict = val_df.to_dict(orient='list')\n",
    "\n",
    "# Fit the label encoder on the labels in the training dataset\n",
    "# Transform the string labels into integer representations\n",
    "\n",
    "label_encoder.fit(train_dict['label'])\n",
    "train_dict['label'] = label_encoder.transform(train_dict['label'])\n",
    "\n",
    "label_encoder.fit(test_dict['label'])\n",
    "test_dict['label'] = label_encoder.transform(test_dict['label'])\n",
    "\n",
    "label_encoder.fit(val_dict['label'])\n",
    "val_dict['label'] = label_encoder.transform(val_dict['label'])\n",
    "\n",
    "\n",
    "train_dict['label'] = [int(x) for x in train_dict['label']]\n",
    "test_dict['label'] = [int(x) for x in test_dict['label']]\n",
    "val_dict['label'] = [int(x) for x in val_dict['label']]\n",
    "\n",
    "\n",
    "# Create a Dataset from the dictionary\n",
    "train_dataset = datasets.Dataset.from_dict(train_dict)\n",
    "test_dataset = datasets.Dataset.from_dict(test_dict)\n",
    "val_dataset = datasets.Dataset.from_dict(val_dict)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "print(val_dataset)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "def tokenize_and_encode_labels(examples):\n",
    "    tokenized_examples = tokenizer(examples['text'], truncation=True, padding=True)\n",
    "    tokenized_examples['input_ids'] = tokenized_examples['input_ids']\n",
    "    tokenized_examples['attention_mask'] = tokenized_examples['attention_mask']\n",
    "    tokenized_examples['label'] = examples['label']\n",
    "    return tokenized_examples\n",
    "\n",
    "train_dataset.map(tokenize_and_encode_labels, batched=True)\n",
    "test_dataset.map(tokenize_and_encode_labels, batched=True)\n",
    "val_dataset.map(tokenize_and_encode_labels, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a46d550-8f36-4178-a048-c758f476d5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = list(df['text'])\n",
    "labels = list(df['label'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "labels_encoded = list(label_encoder.transform(labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8fa66b48-3a4d-4b0c-86df-6fd29cf08c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Train test split \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "tweets_train, tweets_test, labels_train, labels_test = train_test_split(tweets, labels_encoded, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e10021ca-6a7e-4a7f-b889-fde05052ad6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenize \n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "train_encodings = tokenizer(tweets_train, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(tweets_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ba5f192-ac9e-4d95-867a-efda2c54cd19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d455c88d-2f2f-4522-a296-d153e4ad742c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = Dataset(train_encodings, labels_train)\n",
    "test_dataset = Dataset(test_encodings, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98062f90-0fb7-493f-b39b-4b5bc93bba3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert encodings to Dataset object\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']), torch.tensor(train_encodings['attention_mask']), torch.tensor(labels_train))\n",
    "test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']), torch.tensor(test_encodings['attention_mask']), torch.tensor(labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62a60e8c-50c1-40ab-86f3-aa2fd0b71453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    print(type(p))\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37ba2c43-f9e4-4ab8-9065-a2ab2aaa5633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print test dataset\n",
    "\n",
    "#print(\"Test Dataset:\")\n",
    "#for sample in test_dataset:\n",
    "#    input_ids, attention_mask, label = sample\n",
    "#    print(\"Input IDs:\", input_ids)\n",
    "#    print(\"Attention Mask:\", attention_mask)\n",
    "#    print(\"Label:\", label)\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "030da6da-5582-4f05-a83c-26ff535fa0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3744da1-336b-489e-8c22-0f5635c7fca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # output directory\n",
    "    num_train_epochs=EPOCHS,                  # total number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=100,                          # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    logging_dir='./logs',                     # directory for storing logs\n",
    "    logging_steps=160,                         # when to print log\n",
    "    evaluation_strategy='steps',              # evaluate every n number of steps. \n",
    "    eval_steps=160,                            # how often to evaluate. If not set defaults to number of logging_steps\n",
    "    load_best_model_at_end=True,              # to load or not the best model at the end\n",
    "    save_steps=160,                            # create a checkpoint every time we evaluate,\n",
    "    seed=seed                                 # seed for consistent results\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbccd81-a743-43da-adc0-e0f0df60cf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a98ca7c2-665c-4865-8873-7c4c69bf0358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sep2022 were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sep2022 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification,  Trainer, TrainingArguments, RobertaConfig\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL, num_labels=32)\n",
    "trainer = Trainer(\n",
    "    model=model,                              # the instantiated ü§ó Transformers model to be trained\n",
    "    tokenizer=tokenizer,                      # tokenizer to be used to pad the inputs \n",
    "    args=training_args,                       # training arguments, defined above\n",
    "    train_dataset=train_dataset,              # training dataset\n",
    "   # eval_dataset=val_dataset,                  # evaluation dataset\n",
    "    callbacks = [EarlyStoppingCallback(3, 0.001)], # early stopping which stops the training after 3 evaluation calls with no improvement of performance of at least 0.001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4ebdb8a5-a505-479d-844f-3e892d14f901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8\n",
    "\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46723712-50f4-49ea-b12f-056e17acec15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 23/264 03:35 < 41:11, 0.10 it/s, Epoch 0.25/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a28de8-a4de-4613-afa7-e756a3c870c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, RobertaConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the tokenizer and model configuration\n",
    "tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sep2022')\n",
    "config = RobertaConfig.from_pretrained('cardiffnlp/twitter-roberta-base-sep2022')\n",
    "NUM_LABELS = 32  # Replace with the actual number of labels in your dataset\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=NUM_LABELS)\n",
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./output',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                              # the instantiated ü§ó Transformers model to be trained\n",
    "    tokenizer=tokenizer,                      # tokenizer to be used to pad the inputs \n",
    "    args=training_args,                       # training arguments, defined above\n",
    "    train_dataset=train_dataset,              # training dataset\n",
    "   # eval_dataset=val_dataset,                  # evaluation dataset\n",
    "   # callbacks = [EarlyStoppingCallback(3, 0.001)], # early stopping which stops the training after 3 evaluation calls with no improvement of performance of at least 0.001\n",
    ")\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned model on the test dataset\n",
    "test_encodings = tokenizer(test_dataset['text'], truncation=True, padding=True)\n",
    "test_labels = test_dataset['label']\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                                              torch.tensor(test_encodings['attention_mask']),\n",
    "                                              torch.tensor(test_labels))\n",
    "test_loader = DataLoader(test_dataset, batch_size=training_args.per_device_eval_batch_size)\n",
    "result = trainer.evaluate(eval_dataloader=test_loader)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2d840-f40a-4522-ae4a-cab965a1695c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223148d-c1ef-417a-b465-423ac8024c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
